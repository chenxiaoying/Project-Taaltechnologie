\documentclass[a4paper,11pt]{article}
\usepackage[english]{babel}
\usepackage[round,longnamesfirst]{natbib}

\title{ {\small Practical Language Technology:}\\{\Large a Q/A System for DBpedia}\\{\small with a Focus on The Olympic Games}}
\date{\today}
\author{
  {\small Group 13: }
  {\small Xiaoying Chen}{\tiny(s2714140)},
  {\small Roald Baas}{\tiny(s1879642)},
  {\small Anne Wanningen}{\tiny(s2219832)}
}


\begin{document}

%The report should be approx. 5 pages and should ideally address the following topics:

%    A description of dbpedia
%    A description of the task (question answering, what kind of questions)
%    A description of the architecture and major componets of your QA-system
%    Additional knowledge resources used (if any)
%    Results on the test questions
%    Error analysis
%    Division of work in your team (who contributed to what)
%    Each group only submits a single report

\maketitle

\section{Introduction}
% Description of the task
% Description of dbpedia (use of dbpedia to do task), dbpedia citation?
% Next, we describe the architecture of the system
Our task for the course was to build a Dutch question-answer system with the theme: Olympic Games. The Dutch DBpedia (http://nl.dbpedia.org) is used as a source of information in answering questions. DBpedia is a project which extracts machine readable information from Wikipedia, making it available for free as Linked Open Data. The information (which are objects, concepts, resources) on DBpedia are identified by a URI (which often follow the syntax of URL's), which can easily be accessed via an internet browser or using a query language such as SPARQL. It supports 111 different languages, of which the English version is the largest. In our Q/A system we limit ourselves to the Dutch version.



% Meer over de taak; wat voor soorten vragen kunnen we verwachten?
The system is evaluated using 50 test questions, which were unknown at the time of constructing the system. The questions were expected to be of the following types:

\begin{itemize}
\item 'Wie/wat is de/het X van Y?'-questions
\item List-questions (Welke landen...)
\item Count-questions (Hoeveel landen...)
\end{itemize}

The questions were also expected to have the following properties:

\begin{itemize}
\item Questions will be in Dutch
\item Questions will not contain spelling errors
\item Questions are about the Olympics
\item No questions that require context
\item No 'double' questions (Waar en in welk jaar...)
\item No 'or' questions (Waren de Olympische Spelen in 2002 zomer of winter spelen?)
\end{itemize}

Ofcourse the questions may be phrased differently than in these types (for instance 'Door wie...' instead of 'Wie...'). This means that the system has to start by analyzing each question to extract the relevant information used to answer the question, which will be explained in more detail in the next section.


%Dit stuk meer bij architecture? Eerste deel in ieder geval, gedeelte over de vraagtypen heb ik hierboven anders verwoord
%Using SPARQL query can easily search for relation and entities of concepts in DBpedia. When the QA-system read a question from the input, it should be able to give the correct answer. We expect our QA-system will be able to find different types questions. For example, “Wie is de coach van Micheal Phelps?”. This is a typical Wie/wat is de/het X van Y question. Of course the system was also expected to answer list-questions(e.g. Op welke onderdelen kwam Nicolien Sauerbreij uit op de Olympische Spelen?) and count-questions(e.g Hoeveel landen doen mee aan de Olympische Zomerspelen 2012?). The questions will be ask in different ways that the standard format, so the system has to analyze the question first, and then search for the answer with analyzed information.


\section{Architecture}
The system was built in Python3 (http://www.python.org) using the libraries lxml and SPARQLWrapper. We make use of the files \emph{pairCounts} and \emph{similarwords} provided by Spotlight \citep{isem2013daiber} to determine dbpedia URI's and properties of questions. Alpino is used to parse questions, giving XML data as output. XPath is used to extract keywords from this XML data. Finally, after having analysed the question, SPARQL is used to query DBpedia, possibly returning an answer.

Since the system is limited by only answering questions about the Olympics in Dutch and due to the machine readable nature of DBpedia, the file pairCounts (the Dutch version, as other languages are available) can be used to determine URI's. The file similarwords gives us the ability to generate words similar to a given word (as the name suggests).

The input is a text file containing questions (including question number, separated by tabs). The system iterates over this file, analyzing and answering each question, giving the output in a text file using the same format: question number followed by the answer(s), separated by tabs.

The system uses two methods to try and find answers to questions: a fast method and a slower more thorough method.

\subsection{Method 1}
The fast method starts by determining the question type ('wat', 'wie', 'waar', 'wanneer', 'hoeveel', 'welke', 'hoe'). There are slight individual differences between these questions. For example, if the question type 'hoe' also contains the word 'lang', we can add 'lengte' to the list of properties to check (meaning the system will find lengths of athletes).

The method then gets the property and the URI from which to find an answer. The subject of the question is used as a property, and the object is used to find the URI (using pairCounts). For example, in a 'Wie/wat is de/het X van Y?'-question, X is the subject, and Y the object. The property and URI are then used in a SPARQL query to find the information stored in the found property of the found URI.

Since this method is rather quick and dirty, it won't find many answers (but those that it does, it finds quickly and accurately as we will see in the Results section). If this method fails to produce an answer, the system tries again using the second (slower) method.

\subsection{Method 2}
% Second slower method



\section{Results}
% Results
% 41 questions answered, 15 correct. Precision, recall, etc.
Of the 50 test questions, the system found possible answers for 41 questions. Only 15 of those were correct. Method 1 had found 5 correct answers, and method 2 10. The precision for method 1 was 0.83 and 0.22 for method 2. The recall for method 1 and 2 were respectively 0.12 and 0.80.  If we compare the two methods, we noticed that method 1 had a higher precision, and method 2 has a better recall. Based on this data we can conclude that method 2 performed better than method 1 in finding answers, but the answers given by method 1 were generally more accurate. The precision for our overall system (where method 1 and 2 are combined) was 0.366, our recall was 0.3 and our f-score was 0.33. The use of the second method does greatly reduce precision (and thus our f-score), but since the goal was to give as many answers as possible this was not a great problem.

As mentioned before, the first method is faster. The mean time to answer a question in method 1 was 2.7 seconds (with a standard deviation of 1.2 seconds), whereas that of method 2 was 15.9 seconds (with a standard deviation of 20.7 seconds). This large increase in time is due to the system not being able to find an answer immediately, which is made apparent by the fact that if an answer was correctly answered, the mean time to answer is just 6 seconds (with standard deviation 3.4 seconds). In order to increase the speed of the system, we could have implemented a cut-off point, meaning that when it takes too long to find an answer, we conclude that no answer was found. %TODO: misschien dit laatste anders verwoorden, of aan het einde wat suggesties doen voor mogelijke verbeteringen.

The questions answered correctly by the system were the following:

\begin{itemize}
\item 1: Wat is de geboorteplaats van Inge de Bruijn?
\item 2: Wat is de geboortedatum van Leontien van Moorsel?
\item 3: Wat is de bijnaam van Leontien van Moorsel?
\item 7: Wat is de volledige naam van Mark Tuitert?
\item 8: Wat is de specialisatie van Gerard van Velde?
\item 9: Op welke onderdelen kwam Nicolien Sauerbreij uit op de Olympische Spelen?
\item 13: Door wie werden de Olympische Winterspelen 2014 geopend?
\item 16: Wie opende de Paralympische Spelen 2012?
\item 21: Wie is de voorzitter van het NOC*NSF?
\item 22: Wat is de website van het NOC*NSF?
\item 25: Wanneer beginnen de Olympische Zomerspelen van 2016?
\item 29: Hoeveel landen deden mee aan de zomerspelen van 2000?
\item 35: Wanneer werd Churandy Martina geboren?
\item 45: Wat is het motto van de winterspelen in Sotsji?
\item 48: Wie is de coach van het Nederlands hockeyteam?
\end{itemize}

Most of the questions for which our system found a correct answer were with the questions with the format: 'Wie/Wat is de/het X van Y' (questions 1, 2, 3, 7, 8, 21, 22, 45, 48). In order to find answers for this type basic questions, property and URI are only needed (as mentioned before). Finding the property (X) and URI (Y) in cases like this proved to be no problem, which resulted in a high performance for these questions.

Other notable answers we found were that of question 9 (which sports did someone participate in), questions concerning the opener of the Olympics (13 and 16), startdates (25), the number of participating countries (29) and birthdates (35). Question 45 deserves a special mention, since the system was able to determine the URI 'Olympische\_Winterspelen\_2014' based on the object 'winterspelen in Sotsji'.

% some additional data:
%Time M1:
%mean:   2.699058
%sd:     1.212297
%
%Time M2:
%mean:   15.93833
%sd:     20.65463
%
%
%Time M1 Correct:
%mean:   2.293303
%sd:     0.7760793
%
%Time M2 Correct:
%mean:   6.036869
%sd:     3.409159
%
%
%Precision 1:    0.8333333
%Precision 2:    0.2272727
%
%Recall 1:       0.12
%Recall 2:       0.7954545


\subsection{Error analysis}
% What went wrong
The system failed to answer a lot of questions correctly. Some questions were asked in a different way which meant that the system could not answer them. Another question contained a spelling error, causing the system to fail to answer it correctly. We have identified the main points which could be improved in the system as follows:% There are many reasons that caused to this failure.

\begin{enumerate}
\item One single type SPARQL-query for all questions \\
There are questions that give a property and the value of that property, which ask for the URI. Take question 34 for example: 'Wiens bijnaam is Lightning Bolt?'. This is a question that our system could answer correctly, while it can actually be easy to answer. After Alpino has parsed the question and found the property and property value, we could search for the value 'Lightning Bolt' in the 'bijnaam' property in a list of Olympic athletes.

In the first method, we only made a single SPARQL query to find an answer. This could have been made more robust by trying different queries instead of immediately switching to the second method. The first method could have had some expansions to more question types, and an increase in queries by adding a dictionary to try more than one property.

%One mistake we had during building the system was that we did not try multiple possible queries to search for answer. In our system, we just matched our property and URI into one single query in method 1, if that did not work, we switch to method 2. While the system performed method 2, it still matched different properties and URI to a single SPARQL-query. If we designed multiple different SPARQL-queries corresponding to more question types, and let the property, property-value and URI loop through these SPARQL-queries, we could have answer more questions correct. At least if we make corresponding queries for revers question, it is possible to answer this kind questions right.

\item Difficulty on finding correct URI by method 1 \\
When the phrase 'Olympische Zomerspelen' or similar phrases appeared in a question, the system uses it as URI in the SPARQL-query. This however is not always the correct URI for the query. For example, in question 10: 'Hoeveel Nederlandse sporters deden mee aan de Olympische Winterspelen van 2002?', in method 1, the system gets the URI 'Olympische\_Winterspelen\_2002', where it should be 'Nederland\_op\_de\_Olympische\_Winterspelen\_2002'. When a country name is mentioned in the question, most of the time the correct URI should be in this format in order to find the correct answer. Our system did not perform this check. This automatically means questions of this type could not be answered correctly. 
\end{enumerate}


\section{Division of labour}
The division of work was equal. Every member of our group contributed to the construction and testing of the system, the presentation, and the report.


\bibliographystyle{plainnat}
\bibliography{literature}


\end{document}
