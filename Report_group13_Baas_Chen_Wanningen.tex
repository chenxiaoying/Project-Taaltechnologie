\documentclass[a4paper,11pt]{article}
\usepackage[english]{babel}
\usepackage[round,longnamesfirst]{natbib}

\title{ {\small Practical Language Technology:}\\{\Large a Q/A System for DBpedia}\\{\small with a Focus on The Olympic Games}}
\date{\today}
\author{
  {\small Group 13: }
  {\small Xiaoying Chen}{\tiny(s2714140)},
  {\small Roald Baas}{\tiny(s...)},
  {\small Anne Wanningen}{\tiny(s2219832)}
}


\begin{document}

%The report should be approx. 5 pages and should ideally address the following topics:

%    A description of dbpedia
%    A description of the task (question answering, what kind of questions)
%    A description of the architecture and major componets of your QA-system
%    Additional knowledge resources used (if any)
%    Results on the test questions
%    Error analysis
%    Division of work in your team (who contributed to what)
%    Each group only submits a single report

\maketitle

\section{Introduction}
% Description of the task
% Description of dbpedia (use of dbpedia to do task), dbpedia citation?
% Next, we describe the architecture of the system
Our task for the course was to build a Dutch question-answer system with the theme of Olympics Games based on Dutch Dbpedia. DBpedia is a project which extract information from Wikipedia and is free for use. Even though DBpedia has 111 different language edition, which English edition is the largest. But since we had to built a Dutch QA-system, we will use the Dutch edition DBpedia. Using SPARQL query can easily search for relation and entities of concepts in DBpedia. When the QA-system read a question from the input, it should be able to give the correct answer. We expect our QA-system will be able to find different types questions. For example, “Wie is de coach van Micheal Phelps?”. This is a typical Wie/wat is de/het X van Y question. Of course the system was also expected to answer list-questions(e.g. Op welke onderdelen kwam Nicolien Sauerbreij uit op de Olympische Spelen?) and count-questions(e.g Hoeveel landen doen mee aan de Olympische Zomerspelen 2012?). The questions will be ask in different ways that the standard format, so the system has to analyze the question first, and then search for the answer with analyzed information.


\section{Architecture}
% Use of pairCounts + similarwords
%TODO: alpino citation? xpath citation?
The system was built in Python3 (http://www.python.org) using the libraries lxml and SPARQLWrapper. We make use of the files \emph{pairCounts} and \emph{similarwords} provided by Spotlight \citep{isem2013daiber} to determine dbpedia URIs and properties of questions. Alpino is used to parse questions, on which in turn xpath is used to extract keywords.

%TODO: SPARQL gedeelte

% Major components, how everything works, xpath SPARQL etc.

\subsection{Method 1}
% First faster method


\subsection{Method 2}
% Second slower method



\section{Results}
% Results
% 41 questions answered, 15 correct. Precision, recall, etc.
We run our QA-system on the 50 test questions, we found answers for 41 questions, but only 15 answers were correct. Method 1 had found 5 correct answers and 10 correct answers for method 2. The precision for method 1 was 0.83 and for method 2 was 0.22. The recall for method 1 and 2 were respectively 0.12 and 0.80.  If we compare this two methods, we noticed that even method 1 has a higher precision, but method 2 has a better recall, so we can conclude that method 2 performed better than method 1 when finding the correct answers. The precision for our system was 0.366, our recall was 0.3 and our f-score was 0.33.

Most of the questions our system found a correct answers were with the question format: Wie/Wat is de/het X van Y. In order to find answers for this type basic questions, property and URI are only needed. Once the property(X) and URI(Y) are found in the sentence, it can simply apply to the SPARQL-queries and search for the correct answer in dbpedia.

We also find correct answer for the question that ask which sport did someone participated(question9), search for the person who open the game (question 13 and 16), when did the game started(question25), how many country were participated in the game(question29) and when did somebody born.


% some additional data:
%Time M1:
%mean:   2.699058
%sd:     1.212297
%
%Time M2:
%mean:   15.93833
%sd:     20.65463
%
%
%Time M1 Correct:
%mean:   2.293303
%sd:     0.7760793
%
%Time M2 Correct:
%mean:   6.036869
%sd:     3.409159
%
%
%Precision 1:    0.8333333
%Precision 2:    0.2272727
%
%Recall 1:       0.12
%Recall 2:       0.7954545


\subsection{Error analysis}
% What went wrong
We fails to answer a lot of questions correctly. The questions were ask in different ways and our system was not smart enough to answer all of them. There are many reasons that caused to this failure.

\begin{enumerate}
\item One single type SPARQL-query for all questions \\
There are questions that give a property and the value of that property, which ask for the URI. Take question 34 as example, “Wiens bijnaam is Lightning Bolt?”. This is a type question that our system could not find correct answer. But actually, this can be easy to answer. After Alpino had parse the question and find the property and property value, make the corresponding SPARQL-queries will successfully get the correct answer. One mistake we had during building the system was that we did not try multiple possible queries to search for answer. In our system, we just matched our property and URI into one single query in method 1, if that did not work, we switch to method 2. While the system performed method 2, it still matched different properties and URI to a single SPARQL-query. If we designed multiple different SPARQL-queries corresponding to more question types, and let the property, property-value and URI loop through these SPARQL-queries, we could have answer more questions correct. At least if we make corresponding queries for revers question, it is possible to answer this kind questions right.

\item Difficulty on finding correct URI by method 1 \\
When the phrase “Olympische Zomerspelen” or similar phrase appear in the sentence, we will use it as URI in the SPARQL-query. But it is not always the correct URI for the query. For example, question 10 of the test question: “Hoeveel Nederlandse sporters deden mee aan de Olympische Winterspelen van 2002?” When we perform method 1, the system get the URI as “Olympische Winterspelen 2002“, where it should be “Nederland\_op\_de\_Olympische\_Winterspelen\_2002“. When a country name was mentioned in the question, most of the time the correct URI should be in this format for finding the correct answer. But our system did not check if the question contain a country name in order to generate the correct the URI. So we fails to answer the questions when it ask for how many athletes come from a country or who carries the flag for a country.
\end{enumerate}


\bibliographystyle{plainnat}
\bibliography{literature}


\end{document}
